26^280
26^2
26^20
26^250
26^50
500000 * 150
500000 ^15
#You can also embed plots and graphics, for example:
x <- c(1, 3, 4, 5) #this means you "assign" c(1, 3, 4, 5) to an object called x
y <- c(2, 6, 8, 10)
plot(x, y)
#----------------------------------------
# 1 Set up environment                   ---
#----------------------------------------
# clear global environment
rm(list = ls())
# set path where our data are stored
setwd("C:/Users/Kevin/Documents/GitHub/TAD22/")
# load required libraries
library(quanteda)
library(dplyr)
#----------------------------------------
# 2 Load data: conservative manifestos ---
#----------------------------------------
# read in the files
filenames <- list.files(path = "conservative_manifestos", full.names=TRUE)
cons_manifestos <- lapply(filenames, readLines)
cons_manifestos <- unlist(lapply(cons_manifestos, function(x) paste(x, collapse = " "))) # because readLines returns a vector with each elements = lines
cons_manifestos[1]
# construct data frame
manifestos_df <- data.frame( text = cons_manifestos, stringsAsFactors = F)
# Example: Laver Garry dictionary
# https://rdrr.io/github/kbenoit/quanteda.dictionaries/man/data_dictionary_LaverGarry.html
# https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/laver-garry-dictionary-of-policy-position/
lgdict <- dictionary(file = "hu_liu.cat", format = "wordstat",encoding = "utf-8")
# What's in this thing?
lgdict
# Run the conservative manifestos through this dictionary
manifestos_lg <- dfm(manifestos_df$text, dictionary = lgdict)
# how does this look
as.matrix(manifestos_lg)[1:5, 1:5]
featnames(manifestos_lg)
# plot it
plot(     manifestos_lg[,"CULTURE.SPORT"],
xlab="Year", ylab="SPORTS", type="b", pch=19)
# plot it
plot(     manifestos_lg[,"POSITIVE.improve"],
xlab="Year", ylab="SPORTS", type="b", pch=19)
?dictionary
?`dictionary2-class`
# Example: Laver Garry dictionary
# https://rdrr.io/github/kbenoit/quanteda.dictionaries/man/data_dictionary_LaverGarry.html
# https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/laver-garry-dictionary-of-policy-position/
lgdict <- dictionary(file = "hu_liu.cat", format = "wordstat",encoding = "utf-8")
# What's in this thing?
lgdict
# Run the conservative manifestos through this dictionary
manifestos_lg <- dfm(manifestos_df$text, dictionary = lgdict)
# how does this look
as.matrix(manifestos_lg)[1:5, 1:5]
featnames(manifestos_lg)
# plot it
plot(     manifestos_lg[,"POSITIVE.improve"],
xlab="Year", ylab="SPORTS", type="b", pch=19)
# plot it
plot(     manifestos_lg[,"POSITIVE],
xlab="Year", ylab="SPORTS", type="b", pch=19)
# plot it
plot(     manifestos_lg[,"POSITIVE"],
xlab="Year", ylab="SPORTS", type="b", pch=19)
# plot it
plot(     manifestos_lg[,"POSITIVE"]- manifestos_lg[,"NEGATIVE"],
xlab="Year", ylab="SPORTS", type="b", pch=19)
plot(      manifestos_lg[,"NEGATIVE"],
xlab="Year", ylab="SPORTS", type="b", pch=19)
plot(      manifestos_lg[,"POSITIVE"],
xlab="Year", ylab="SPORTS", type="b", pch=19)
# load required libraries
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
install.packages("ggplot2")
library(caret)
library(ggplot2)
install.packages("rlang")
install.packages("rlang")
library(ggplot2)
library(caret)
install.packages("recipes")
library(ggplot2)
library(caret)
# set working directory
setwd("C:/Users/kevin/Documents/GitHub/TAD22/")
#----------------------------------------
# 1. Load, clean and inspect data        ---
#----------------------------------------
news_data <- readRDS("news_data.rds")
table(news_data$category)
# let's work with 2 categories
set.seed(1984)
news_samp <- news_data %>%
filter(category %in% c("MONEY", "LATINO VOICES")) %>%
group_by(category) %>%
sample_n(500) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
# load required libraries
library(dplyr)
library(randomForest)
library(mlbench)
#----------------------------------------
# 1. Load, clean and inspect data        ---
#----------------------------------------
news_data <- readRDS("news_data.rds")
table(news_data$category)
# let's work with 2 categories
set.seed(1984)
news_samp <- news_data %>%
filter(category %in% c("MONEY", "LATINO VOICES")) %>%
group_by(category) %>%
sample_n(500) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
# get a sense of how the text looks
dim(news_samp)
head(news_samp$text[news_samp$class == "MONEY"])
head(news_samp$text[news_samp$class == "LATINO VOICES"])
# some pre-processing (the rest we'll let dfm do)
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- recode(news_samp$class,  "MONEY" = "money", "LATINO VOICES" = "latino")
# what's the distribution of classes?
prop.table(table(news_samp$class))
# randomize order (notice how we split below)
set.seed(1984)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
library(quanteda)
# create document feature matrix
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
# keep tokens that appear in at least 5 headlines
presen_absent <- news_dfm
presen_absent[presen_absent > 0] <- 1
feature_count <- apply(presen_absent, 2, sum)
features <- names(which(feature_count > 5))
news_dfm <- news_dfm[,features]
# caret package has it's own partitioning function
set.seed(1984)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
#----------------------------------------
# 3. Using RandomForest                  ---
#----------------------------------------
library(randomForest)
mtry = sqrt(ncol(train_x))  # number of features to sample at each split
ntree = 51  # numbre of trees to grow
# more trees generally improve accuracy but at the cost of computation time
# odd numbers avoid ties (recall default aggregation is "majority voting")
set.seed(1984)
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
# print results
print(rf.base)
# plot importance
# gini impurity = how "pure" is given node ~ class distribution
# = 0 if all instances the node applies to are of the same class
# upper bound depends on number of instances
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
# evaluate on test set
predicted_class_sm <- predict(rf.base, newdata = test_dfm)
# evaluate on test set
predicted_class_sm <- predict(rf.base, newdata = test_x)
# get confusion matrix
cmat_sm <- table(test_set$class, predicted_class_sm)
# get confusion matrix
cmat_sm <- table(test_y, predicted_class_sm)
nb_acc_sm <- sum(diag(cmat_sm))/sum(cmat_sm) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall_sm <- cmat_sm[2,2]/sum(cmat_sm[2,]) # recall = TP / (TP + FN)
nb_precision_sm <- cmat_sm[2,2]/sum(cmat_sm[,2]) # precision = TP / (TP + FP)
nb_f1_sm <- 2*(nb_recall_sm*nb_precision_sm)/(nb_recall_sm + nb_precision_sm)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
# print
cat(
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
## load rtweet package
library(rtweet)
app_name <- "smapp"
tmls$favorite_count
## copy and pasted *your* keys (these are fake)
consumer_key <- "qYQF4B3mp28MAn74iIEc4Og2e"
consumer_secret <- "NW8PATIAzBZt0cAsvm4Ab3vTPDInHMjmbnz2JotisNRLoAXSJR"
access_token <- "89098361-MCBLt2hS68KpEzFUWYuqNi1jBuPrEipWk22rK9yos"
access_secret <- "Tk6k1WGD9ouzZcM4dIhL8q4cdD4Ain1WUD6w3raVEnrtr"
vignette('auth')
auth <- rtweet_app()
auth
mcs<-c("@AOC", "@BernieSanders", "@EWarren", "@SpeakerPelosi", "@CoryBooker",
"@TedCruz", "@RandPaul", "@MarcoRubio", "@dancrenshawtx", "@MittRomney")
tmls <- get_timelines(mcs, n = 200, token = token)
tmls <- get_timelines(mcs, n = 200, token = auth)
docs<-aggregate(text ~ user_id, data = tmls, paste, collapse = " ")
docs<-aggregate(text ~ id, data = tmls, paste, collapse = " ")
docs$class<-c(rep("D", 5), rep("R", 5))
table(tmls$id_str)
table(tmls$entities[[1]]$user_mentions)
table(tmls$source)
table(tmls$contributors)
users_data(tmls)
xx<-users_data(tmls)
library(dplyr)
xx<-dplyr::select(c("id_str", "screen_name"), users )
users<-users_data(tmls)
xx<-dplyr::select(c("id_str", "screen_name"), users )
?select
xx<-dplyr::select(("id_str", "screen_name"), users )
xx<-dplyr::select(id_str, screen_name, users )
xx<-dplyr::select(users, screen_name, id_str )
xxx<-merge(x = tmls, y = xx, by = id_str )
xxx<-merge(x = tmls, y = xx, by = "id_str" )
xxx<-merge(x = tmls, y = xx, by.y = "id_str", by.x = "id_str" )
users
tmls
table(users$id_str)
tmls <- get_timelines(mcs, n = 10, token = auth)
users<-users_data(tmls)
table(users$aoc)
users<-users_data(tmls)
table(users$screen_name)
tmls <- get_timelines(mcs, n = 200, token = auth)
users<-users_data(tmls)
mcs_dem<-c("@AOC", "@BernieSanders", "@EWarren", "@SpeakerPelosi", "@CoryBooker")
mcs_rep<-c("@TedCruz", "@RandPaul", "@MarcoRubio", "@dancrenshawtx", "@MittRomney")
mcs_dem<-c("@AOC", "@BernieSanders", "@EWarren", "@SpeakerPelosi", "@CoryBooker")
mcs_rep<-c("@TedCruz", "@RandPaul", "@MarcoRubio", "@dancrenshawtx", "@MittRomney")
tmls_dem <- get_timelines(mcs_dem, n = 200, token = auth)
tmls_rep <- get_timelines(mcs_rep, n = 200, token = auth)
docs<-data.frame(text = c(paste(tmls_dem$text), collapse = ' ') ,
paste(tmls_dem$text), collapse = ' ') ,
docs<-data.frame(text = c(paste(tmls_dem$text), collapse = ' ') ,
paste(tmls_dem$text), collapse = ' ')) ,
docs<-data.frame(text = c(paste(tmls_dem$text), collapse = ' ') ),
docs<-data.frame(text = c(paste(tmls_dem$text), collapse = ' ') ,
paste(tmls_dem$text), collapse = ' '),
data.frame
?data.frame
docs<-data.frame(text = c(paste(tmls_dem$text, collapse = ' ') ,
paste(tmls_dem$text, collapse = ' ')),
class = c("D", "R"))
xdfm <-dfm(docs$text, stem = F, remove_punct = T, tolower = T, remove_numbers = TRUE,
remove = c(stopwords("english"),"¦","€","¥", "¯","~", "http","https","rt", "t.co"))
library(quanteda)
xdfm <-dfm(docs$text, stem = F, remove_punct = T, tolower = T, remove_numbers = TRUE,
remove = c(stopwords("english"),"¦","€","¥", "¯","~", "http","https","rt", "t.co"))
nb_model <- textmodel_nb(xdfm, docs$class, smooth = 1, prior = "uniform")
library(quanteda.textmodels)
nb_model <- textmodel_nb(xdfm, docs$class, smooth = 1, prior = "uniform")
posterior <- data.frame(feature = rownames(t(nb_model$param)),
post_D = t(nb_model$param)[,1],
post_R = t(nb_model$param)[,2])
head(arrange(posterior, -post_D))
head(arrange(posterior, -post_R), n = 20)
## load rtweet package
library(rtweet)
library(dplyr)
app_name <- "smapp"
## copy and pasted *your* keys (these are fake)
consumer_key <- "qYQF4B3mp28MAn74iIEc4Og2e"
consumer_secret <- "NW8PATIAzBZt0cAsvm4Ab3vTPDInHMjmbnz2JotisNRLoAXSJR"
access_token <- "89098361-MCBLt2hS68KpEzFUWYuqNi1jBuPrEipWk22rK9yos"
access_secret <- "Tk6k1WGD9ouzZcM4dIhL8q4cdD4Ain1WUD6w3raVEnrtr"
vignette('auth')
auth <- rtweet_app()
auth
mcs<-c("@AOC", "@BernieSanders", "@EWarren", "@SpeakerPelosi", "@CoryBooker",
"@TedCruz", "@RandPaul", "@MarcoRubio", "@dancrenshawtx", "@MittRomney")
mcs_dem<-c("@AOC", "@BernieSanders", "@EWarren", "@SpeakerPelosi", "@CoryBooker")
mcs_rep<-c("@TedCruz", "@RandPaul", "@MarcoRubio", "@dancrenshawtx", "@MittRomney")
tmls_dem <- get_timelines(mcs_dem, n = 200, token = auth)
tmls_rep <- get_timelines(mcs_rep, n = 200, token = auth)
docs<-data.frame(text = c(paste(tmls_dem$text, collapse = ' ') ,
paste(tmls_dem$text, collapse = ' ')),
class = c("D", "R"))
library(quanteda)
xdfm <-dfm(docs$text, stem = F, remove_punct = T, tolower = T, remove_numbers = TRUE,
remove = c(stopwords("english"),"¦","€","¥", "¯","~", "http","https","rt", "t.co"))
library(quanteda)
library(quanteda.textmodels)
nb_model <- textmodel_nb(xdfm, docs$class, smooth = 1, prior = "uniform")
posterior <- data.frame(feature = rownames(t(nb_model$param)),
post_D = t(nb_model$param)[,1],
post_R = t(nb_model$param)[,2])
head(arrange(posterior, -post_D))
head(arrange(posterior, -post_R), n = 20)
docs<-data.frame(text = c(paste(tmls_dem$text, collapse = ' ') ,
paste(tmls_rep$text, collapse = ' ')),
class = c("D", "R"))
xdfm <-dfm(docs$text, stem = F, remove_punct = T, tolower = T, remove_numbers = TRUE,
remove = c(stopwords("english"),"¦","€","¥", "¯","~", "http","https","rt", "t.co"))
nb_model <- textmodel_nb(xdfm, docs$class, smooth = 1, prior = "uniform")
posterior <- data.frame(feature = rownames(t(nb_model$param)),
post_D = t(nb_model$param)[,1],
post_R = t(nb_model$param)[,2])
head(arrange(posterior, -post_D))
head(arrange(posterior, -post_R), n = 20)
ppl<-c("@elonmusk", "@stephenking")
docs<-data.frame(text = c(paste(tmls_dem$text, collapse = ' ') ,
paste(tmls_rep$text, collapse = ' '))
)
ppldfm <-dfm(docs$text, stem = F, remove_punct = T, tolower = T, remove_numbers = TRUE,
remove = c(stopwords("english"),"¦","€","¥", "¯","~", "http","https","rt", "t.co"))
predicted_class <- predict(nb_model, newdata = ppldfm)
predicted_class
dfm_ppl <- dfm_match(ppldfm, features = featnames(xdfm))
predicted_class <- predict(nb_model, newdata = dfm_ppl)
predicted_class
tmls_dem <- get_timelines("@elonmusk", n = 1000, token = auth)
tmls_rep <- get_timelines("@stephenking", n = 1000, token = auth)
docs<-data.frame(text = c(paste(tmls_dem$text, collapse = ' ') ,
paste(tmls_rep$text, collapse = ' '))
)
ppldfm <-dfm(docs$text, stem = F, remove_punct = T, tolower = T, remove_numbers = TRUE,
remove = c(stopwords("english"),"¦","€","¥", "¯","~", "http","https","rt", "t.co"))
dfm_ppl <- dfm_match(ppldfm, features = featnames(xdfm))
predicted_class <- predict(nb_model, newdata = dfm_ppl)
predicted_class
library(topicmodels)
?LDA
library(stm)
?stm
predicted_class <- predict(nb_model, newdata = dfm_ppl)
predicted_class
install.packages("rtweet")
install.packages("rtweet")
library(rtweet)
library(devtools)
install.packages("devtools")
library(devtools)
install_github("ropensci/rtweet@devel")
